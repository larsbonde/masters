{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee71536",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch_geometric\n",
    "import Bio.PDB as PDB\n",
    "import kmbio  # fork of biopython PDB with some changes in how the structure, chain, etc. classes are defined.\n",
    "import numpy as np\n",
    "import proteinsolver\n",
    "\n",
    "from proteinsolver.models.model import *\n",
    "from proteinsolver.datasets import *\n",
    "\n",
    "# custom stuff\n",
    "#import proteinsolver_utils\n",
    "#import proteinsolver_datasets\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f17a38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from_dir = \"/home/sebastian/masters/data/210916_TCRpMHCmodels/models/\"\n",
    "#to_dir = \"/home/sebastian/masters/data/neat_data/tcrpmhc/\"\n",
    "#model_suffix = \"model_TCR-pMHC.pdb\"\n",
    "#for subdir in os.listdir(from_dir):\n",
    "#    subdir_id = subdir.split(\"_\")[0]\n",
    "#    new_name = f\"tcrpmhc_{subdir_id}.pdb\"\n",
    "#    os.system(f\"mv {from_dir}/{subdir}/{model_suffix} {to_dir}/{new_name}\")\n",
    "#    \n",
    "#from_dir = \"/home/sebastian/masters/data/embedding_verification/raw_filtered_models\"\n",
    "#to_dir = \"/home/sebastian/masters/data/neat_data/pmhc/\"\n",
    "#model_suffix = \"model_pMHC.pdb\"\n",
    "#for subdir in os.listdir(from_dir):\n",
    "#    subdir_id = subdir.split(\"_\")[0]\n",
    "#    new_name = f\"pmhc_{subdir_id}.pdb\"\n",
    "#    os.system(f\"mv {from_dir}/{subdir}/{model_suffix} {to_dir}/{new_name}\")\n",
    "#    \n",
    "#from_dir = \"/home/sebastian/masters/data/embedding_verification/raw_filtered_models\"\n",
    "#to_dir = \"/home/sebastian/masters/data/neat_data/p/\"\n",
    "#model_suffix = \"model_p.pdb\"\n",
    "#for subdir in os.listdir(from_dir):\n",
    "#    subdir_id = subdir.split(\"_\")[0]\n",
    "#    new_name = f\"p_{subdir_id}.pdb\"\n",
    "#    os.system(f\"mv {from_dir}/{subdir}/{model_suffix} {to_dir}/{new_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fc67c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNIQUE_ID = \"191f05de\"\n",
    "BEST_STATE_FILES = {\n",
    "    #\n",
    "    \"191f05de\": \"/home/sebastian/proteinsolver/data/e53-s1952148-d93703104.state\"\n",
    "}\n",
    "state_file = BEST_STATE_FILES[UNIQUE_ID]\n",
    "\n",
    "\n",
    "#test_file = \"/home/sebastian/proteinsolver/notebooks/protein_demo/inputs/1n5uA03.pdb\"\n",
    "#test_id = \"1n5uA03.pdb\"\n",
    "\n",
    "test_file = \"/home/sebastian/masters/data/test/3hfm.pdb\"\n",
    "test_id = \"3hfm\"\n",
    "\n",
    "def load_model_paths(data_dir, model=\"model_TCR-pMHC.pdb\"):\n",
    "    model_list = list()\n",
    "    for subdir in os.listdir(data_dir):\n",
    "        path = f\"{data_dir}/{subdir}/{model}\"\n",
    "        model_list.append(path)\n",
    "    return np.array(model_list)\n",
    "\n",
    "infiles = load_model_paths(\"/home/sebastian/masters/data/210916_TCRpMHCmodels/models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb016cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_all = kmbio.PDB.load(infiles[0])\n",
    "structure_all = merge_chains(structure_all)\n",
    "structure = kmbio.PDB.Structure(test_id, structure_all[0].extract('A'))\n",
    "\n",
    "pdata = proteinsolver.utils.extract_seq_and_adj(structure, 'A', remove_hetatms=True)\n",
    "data = proteinsolver.datasets.row_to_data(pdata)\n",
    "data = proteinsolver.datasets.transform_edge_attr(data)\n",
    "data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04fe7c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20d267c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "softmax() missing 1 required positional argument: 'input'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-52a5e47581cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: softmax() missing 1 required positional argument: 'input'"
     ]
    }
   ],
   "source": [
    "F.softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eb50af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_loader = iter(torch_geometric.data.DataLoader(d_train, batch_size=batch_size))\n",
    "#valid_loader = iter(torch_geometric.data.DataLoader(d_valid, batch_size=batch_size))\n",
    "#d = next(train_loader)\n",
    "#x = torch.ones_like(d.x)*d.x.max().item()\n",
    "#out = gnn(x, d.edge_index, d.edge_attr)\n",
    "#\n",
    "#net.eval()\n",
    "#with torch.no_grad():\n",
    "#    y = net(out.T.unsqueeze(0))\n",
    "#    y = F.softmax(y, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31befdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnn_to_fnn(data, hidden_size, gnn_instance):\n",
    "        data = data.to(device)\n",
    "        y = data.y.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = gnn_instance(data.x, data.edge_index, data.edge_attr)\n",
    "        \n",
    "        batches = torch.unique(data.batch)\n",
    "        sliced_outs = list()\n",
    "        pool = nn.AdaptiveAvgPool1d(output_size=hidden_size)\n",
    "        for batch_idx in batches:\n",
    "            batch_slice = torch.nonzero(data.batch == batch_idx)\n",
    "            chain_map = data.chain_map[batch_idx]\n",
    "            out_sliced = out[batch_slice]\n",
    "            out_sliced = out_sliced[chain_map == \"P\"]  # get peptide only\n",
    "            out_sliced = pool(out_sliced.T)\n",
    "            sliced_outs.append(out_sliced)\n",
    "        batched_out = torch.cat(sliced_outs, dim=1)\n",
    "        return out, y\n",
    "    \n",
    "def gnn_to_lstm_batch(data, gnn_instance, device, num_classes):\n",
    "    \"\"\"function for bridging gnn output to lstm\"\"\"\n",
    "    data = data.to(device)\n",
    "    y = data.y\n",
    "    with torch.no_grad():\n",
    "        out = gnn_instance(data.x, data.edge_index, data.edge_attr)\n",
    "    \n",
    "    batches = torch.unique(data.batch)\n",
    "    sliced_embeddings = list()\n",
    "    encoded_y = list()\n",
    "    for batch_idx in batches:\n",
    "        # split sub graphs into batches\n",
    "        batch_slice = torch.nonzero(data.batch == batch_idx)\n",
    "        chain_map = data.chain_map[batch_idx]\n",
    "        one_batch_peptide_emb = out[batch_slice][chain_map == \"P\"]  # get peptide only\n",
    "        sliced_embeddings.append(one_batch_peptide_emb.squeeze(1))\n",
    "        \n",
    "        # one hot encode targets\n",
    "        sliced_y = int(y[batch_idx].item())\n",
    "        one_hot_y = np.zeros(num_classes)\n",
    "        one_hot_y[sliced_y] = 1\n",
    "        encoded_y.append(one_hot_y)\n",
    "        \n",
    "    sliced_embeddings.sort(key=lambda x: len(x))\n",
    "    encoded_y = torch.Tensor(encoded_y)\n",
    "    \n",
    "    return sliced_embeddings, encoded_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbda8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFNN(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(hidden_size * num_features, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)  # output dim should be 3 long\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.T.flatten()\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b496345d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init proteinsolver gnn\n",
    "num_features = 20\n",
    "adj_input_size = 2\n",
    "hidden_size = 128\n",
    "#frac_present = 0.5\n",
    "#frac_present_valid = frac_present\n",
    "#info_size= 1024\n",
    "\n",
    "gnn = Net(\n",
    "    x_input_size=num_features + 1, \n",
    "    adj_input_size=adj_input_size, \n",
    "    hidden_size=hidden_size, \n",
    "    output_size=num_features\n",
    ")\n",
    "gnn.load_state_dict(torch.load(state_file, map_location=device))\n",
    "gnn.eval()\n",
    "gnn = gnn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b171b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import KFold\n",
    "#from sklearn.metrics import *\n",
    "#from torch import nn, optim\n",
    "#import torch.nn.functional as F\n",
    "#\n",
    "#\n",
    "#root = Path(\"/home/sebastian/masters/data/\")\n",
    "#data_root = root / \"neat_data\"\n",
    "#metadata_path = data_root / \"embedding_dataset.csv\"\n",
    "#processed_dir = data_root / \"processed\" / \"embedding_verification\"\n",
    "#state_file = root / \"state_files\" / \"e53-s1952148-d93703104.state\"\n",
    "#out_dir = root / \"state_files\" / \"embedding_verification\" \n",
    "#\n",
    "#device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "#\n",
    "## load dataset\n",
    "#raw_files = list()\n",
    "#targets = list()\n",
    "#with open(metadata_path, \"r\") as infile:\n",
    "#    for line in infile:\n",
    "#        line = line.strip().split(\",\")\n",
    "#        raw_files.append(line[0])\n",
    "#        targets.append(int(line[1]))\n",
    "#\n",
    "#raw_files = np.array(raw_files)\n",
    "#targets = np.array(targets)\n",
    "#\n",
    "#dataset = ProteinDataset(processed_dir, raw_files, targets, overwrite=False)\n",
    "#\n",
    "## init proteinsolver gnn\n",
    "#num_features = 20\n",
    "#adj_input_size = 2\n",
    "#hidden_size = 128\n",
    "#\n",
    "#gnn = Net(\n",
    "#    x_input_size=num_features + 1, \n",
    "#    adj_input_size=adj_input_size, \n",
    "#    hidden_size=hidden_size, \n",
    "#    output_size=num_features\n",
    "#)\n",
    "#gnn.load_state_dict(torch.load(state_file, map_location=device))\n",
    "#gnn.eval()\n",
    "#gnn = gnn.to(device)\n",
    "#\n",
    "## init LSTM\n",
    "#num_classes = 3\n",
    "#num_layers = 2\n",
    "#hidden_size = 26\n",
    "#\n",
    "#net = MyLSTM(num_classes, num_features, num_layers, hidden_size)\n",
    "#net = net.to(device)\n",
    "#\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "#optimizer = optim.Adam(net.parameters(), lr=0.0001) \n",
    "#\n",
    "## training params\n",
    "#epochs = 1\n",
    "#n_splits = 5\n",
    "#batch_size = 5\n",
    "#\n",
    "## touch files to ensure output\n",
    "#save_dir = get_non_dupe_dir(out_dir)\n",
    "#loss_paths = touch_output_files(save_dir, \"loss\", n_splits)\n",
    "#state_paths = touch_output_files(save_dir, \"state\", n_splits)\n",
    "#pred_paths = touch_output_files(save_dir, \"pred\", n_splits)\n",
    "#\n",
    "#CV = KFold(n_splits=n_splits, shuffle=True)\n",
    "#i = 0\n",
    "#for train_idx, valid_idx in CV.split(dataset):\n",
    "#    \n",
    "#    train_subset = dataset[torch.LongTensor(train_idx)][0:10]\n",
    "#    valid_subset = dataset[torch.LongTensor(valid_idx)][0:10]\n",
    "#    \n",
    "#    net = MyLSTM(num_classes, num_features, num_layers, hidden_size)\n",
    "#    net = net.to(device)\n",
    "#    \n",
    "#    # partial function - gnn arg is static, x is given later\n",
    "#    gnn_transform = lambda x: gnn_to_lstm_batch(\n",
    "#        x, \n",
    "#        gnn_instance=gnn, \n",
    "#        device=device,\n",
    "#        num_classes=num_classes\n",
    "#)\n",
    "#    \n",
    "#    net, train_subset_losses, valid_subset_losses = train_model(\n",
    "#        model=net,\n",
    "#        epochs=epochs, \n",
    "#        criterion=criterion,\n",
    "#        optimizer=optimizer,\n",
    "#        train_data=train_subset, \n",
    "#        valid_data=valid_subset,\n",
    "#        batch_size=batch_size,\n",
    "#        device=device,\n",
    "#        transform=gnn_transform,\n",
    "#)\n",
    "#\n",
    "#    torch.save({\"train\": train_subset_losses, \"valid\": valid_subset_losses}, loss_paths[i])\n",
    "#    torch.save(net.state_dict(), state_paths[i])\n",
    "#    \n",
    "#    # perform test preds\n",
    "#    y_pred, y_true = predict(\n",
    "#        model=net, \n",
    "#        data=train_subset, \n",
    "#        batch_size=batch_size,\n",
    "#        device=device,\n",
    "#        transform=gnn_transform,\n",
    "#)\n",
    "#\n",
    "#    torch.save({\"y_pred\": y_pred, \"y_true\": y_true,}, pred_paths[i])\n",
    "#    \n",
    "#    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64886378",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #annotations = train_subset.dataset.annotations.squeeze(1)\n",
    "    #class_weights = compute_class_weight(\n",
    "    #    'balanced',\n",
    "    #    np.unique(annotations),\n",
    "    #    annotations.numpy()\n",
    "    #)\n",
    "    #class_weights = torch.tensor(class_weights, dtype=torch.float)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
